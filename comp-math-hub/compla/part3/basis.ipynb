{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We have now seen that one way to describe a vector space or subspace is to span a set of vectors. Since most spaces are infinitely large, we have infinitely many vectors to choose from. However, the *minimum* number of vectors needed will always be the same for a given space."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Linear Independence\n",
                "\n",
                "We have encountered this idea previously, but we will formally define it now. A set of vectors $\\mathbf v_1, ..., \\mathbf v_n$ is **linearly independent** if no single vector can be written as a linear combination of the others. Alternatively, the only way to obtain $\\mathbf 0$ through linear combinations of $\\mathbf v_1, ..., \\mathbf v_n$ is to set all coefficients to be 0:\n",
                "\n",
                "$$c_1 \\mathbf v_1 + \\cdots + c_n \\mathbf v_n = \\mathbf 0 \\quad \\Rightarrow \\quad c_i = 0 \\, \\forall \\, i$$\n",
                "\n",
                "Let's consider some examples. Every set of a single vector, aside from the zero vector, is linearly independent. The zero vector is the only exception; any set containing it is linearly dependent. A set of two vectors is linearly dependent iff they are multiples of each other; they lie along the same line. Three vectors are linearly dependent if they lie in the same plane. Any set of $n$ $m$-vectors, where $n\u003em$, is linearly dependent.\n",
                "\n",
                "We can now relate this idea to matrices. The vectors $\\mathbf v_1, ..., \\mathbf v_n$ are linearly independent iff the only solution to $A \\mathbf x = \\mathbf 0$, where $A$ is the matrix whose columns are the vectors $\\mathbf v_i$, is $\\mathbf x = \\mathbf 0$. In other words, the columns of $A$ are linearly independent iff $N(A) = \\{\\mathbf 0\\}$ and $A$ is full column rank. A wide matrix cannot be full column rank and thus has linearly dependent columns."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Basis for a Vector Space\n",
                "\n",
                "The idea of linear independence gives us a new way to think about describing vector spaces. There is no need to include a linearly dependent vector in the span of a set of vectors, as it can be formed by a linear combination of the others. A **basis** for a vector space $V$ is a linearly independent set of vectors that spans $V$.\n",
                "\n",
                "$V$ will generally still have infinitely many possible bases that describing it. But given a basis, every vector $\\mathbf b$ in $V$ can be written as a *unique* combination of the basis vectors. To solve for the coefficients, we again solve the linear system $A \\mathbf x = \\mathbf b$, where $A$ is the matrix whose columns are the basis vectors of $V$. \n",
                "\n",
                "For example, the *standard basis vectors* for $\\mathbb R^n$ are the columns of the $n \\times n$ identity matrix $I_n$. If $n=2$ or $n=3$, these are just the standard coordinate $x$, $y$, and $z$ vectors. However, $\\mathbb R^n$ has infinitely many bases; just take the columns of *any* invertible $n \\times n$ matrix. Any $n$-vector can be written as a unique combination of a given matrix's columns.\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Column and Row Space\n",
                "\n",
                "We have already seen how to solve for and in fact find a basis for the null space of a matrix. Our method guarantees that the vectors we extract are linearly independent, because each one contains a $1$ entry in a unique free variable. \n",
                "\n",
                "What about the column and row space, which we wrote as the span of a matrix's columns and rows? These only form bases if the matrix is full column or full row rank, respectively. In particular for a full column rank matrix $A$, the equation $A \\mathbf x = \\mathbf b$ reveals the *coordinates* (coefficients) $\\mathbf x$ of $\\mathbf b$ in the column basis.\n",
                "\n",
                "To find a basis for the column space of a matrix $A$ more generally, we look at the pivot columns in its REF (or RREF) $R$. The free columns are ignored since they can be written as linear combinations of the pivot columns. The corresponding columns *in the original matrix $A$* form a basis for $C(A)$. Note that it is important to go back to the columns of $A$, not $R$. Row operations change the directions of the column vectors.\n",
                "\n",
                "To find a basis for the row space $C(A^\\top)$, you may first transpose $A$ and then perform the same procedure described above. Alternatively, the nonzero rows of $R$ also form a basis for $C(A^\\top)$. Note that *we look in the REF matrix $R$* for the row space basis, not $A$. Row operations may swap rows around, but the spanning set is unchanged.\n",
                "\n",
                "Consider the following example:\n",
                "\n",
                "$$ A = \\begin{bmatrix} 2 \u0026 4 \\\\ 3 \u0026 6 \\end{bmatrix}, \\quad \n",
                "R = \\begin{bmatrix} 1 \u0026 2 \\\\ 0 \u0026 0 \\end{bmatrix} $$\n",
                "\n",
                "$R$ has one pivot entry in the first column and first row. The rank of $A$ and $R$ is $1$; they are not full rank. A basis for $N(A)$ is $\\{(-2,1)\\}$. A basis for $C(A)$ is just the first column of $A$: $\\{(2,3)\\}$. A basis for $C(A^\\top)$ is the first row of $R$: $\\{(1,2)\\}$. All of these spaces are subspaces of $\\mathbb R^2$."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dimension\n",
                "\n",
                "All bases for a vector space $V$ have the same number of vectors. This number is the **dimension** of $V$. For example, every line is 1-dimensional and every plane is 2-dimensional. $\\mathbb R^n$ is $n$-dimensional. By convention, $\\{\\mathbf 0\\}$ is defined to have $0$ dimension.\n",
                "\n",
                "The column and row spaces of a matrix $A$ have dimension $r$, equal to its rank and number of pivots. The **nullity**, or dimension of the null space, of a matrix is equal to the number of free columns, $n-r$. This can be restated as the all-important *rank-nullity theorem*:\n",
                "\n",
                "$$\\text{rank}(A) + \\text{nullity}(A) = n$$\n",
                "\n",
                "This equation can also relate the dimensions of the row space and left null space, or alternatively the column and null spaces of $A^\\top$:\n",
                "\n",
                "$$\\text{rank}(A) + \\text{nullity}(A^\\top) = m$$"
            ]
        }
    ]
}
